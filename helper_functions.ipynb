{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting helper_functions.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile helper_functions.py\n",
    "import smtplib\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import threading\n",
    "from functools import wraps\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import log_loss, accuracy_score, precision_score, recall_score, roc_auc_score,log_loss\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from IPython.core.debugger import Tracer\n",
    "\n",
    "\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.application import MIMEApplication\n",
    "from email.mime.image import MIMEImage\n",
    "\n",
    "from os.path import basename\n",
    "\n",
    "\n",
    "import cProfile\n",
    "import pstats\n",
    "from io import StringIO\n",
    "import marshal\n",
    "import tempfile\n",
    "import pprint\n",
    "import psutil\n",
    "import re\n",
    "\n",
    "import objgraph\n",
    "\n",
    "import pickle\n",
    "from os.path import exists\n",
    "\n",
    "\n",
    "# bit-serialize any object.Creates a new file if none, else appends.\n",
    "# returns pickled object if only path is supplied\n",
    "# stores the object in a dictionary with obj_key as key\n",
    "def pickler(path, obj_to_pickle = None, obj_key = None):\n",
    "\n",
    "\n",
    "    save ={}\n",
    "    \n",
    "    if exists(path):\n",
    "        try:\n",
    "          f = open(path, 'rb')\n",
    "          save = pickle.load(f)\n",
    "          f.close()\n",
    "        except Exception as e:\n",
    "          print('Unable to read data from', context['pickle'], ':', e)\n",
    "          raise\n",
    "\n",
    "    if(obj_to_pickle):\n",
    "        save.update({obj_key: obj_to_pickle})\n",
    "\n",
    "        try:\n",
    "          f = open(path, 'wb')\n",
    "          pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "          f.close()\n",
    "        except Exception as e:\n",
    "          print('Unable to save data to', context['pickle'], ':', e)\n",
    "          raise\n",
    "    \n",
    "    return save\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Writes a dictionary to the file at supplied path.\n",
    "# Optional description text describing the dictionary\n",
    "\n",
    "def write_dict(d, path, description =''):\n",
    "\n",
    "    \n",
    "    with open(path, \"a\") as f:\n",
    "      h_line = '-------------------------------\\n'\n",
    "      f.write(h_line)\n",
    "      f.write(description +'\\n')\n",
    "      for k, v in d.items():\n",
    "        if isinstance(v, dict):\n",
    "          write_dict(v, path)\n",
    "        else:\n",
    "            tmp_str = str(k) + ' : ' + str(v) +'\\n'\n",
    "            f.write(tmp_str)\n",
    "      f.write(h_line)\n",
    "    \n",
    "    return(d)\n",
    "\n",
    "\n",
    "#computes max_memory and cpu usage from dictionary of measured results \n",
    "def max_stats(profile_results, context):\n",
    "    cpu_list= []\n",
    "    used_memory_list =[]\n",
    "    active_memory_list =[]\n",
    "    total_memory_list = []\n",
    "    buffered_memory_list =[]\n",
    "    cached_memory_list =[]\n",
    "    shared_memory_list = []\n",
    "    swap_memory_list = []\n",
    "    return_dict= {}\n",
    "\n",
    "\n",
    "    for i, (key,value) in enumerate(profile_results.items()):\n",
    "\n",
    "        if not key == 'max_memory':\n",
    "            cpu_list.append(value['all_cpu'])             \n",
    "            total_memory_list.append(value['memory'][0])\n",
    "            used_memory_list.append(value['memory'][3])\n",
    "            active_memory_list.append(value['memory'][5])\n",
    "            buffered_memory_list.append(value['memory'][7])\n",
    "            cached_memory_list.append(value['memory'][8])\n",
    "            shared_memory_list.append(value['memory'][9])\n",
    "            swap_memory_list.append(value['swap'][0])\n",
    "            \n",
    "            \n",
    "    max_memory = profile_results['max_memory']  \n",
    "    \n",
    "    return_dict.update({'max_cpu': np.max(cpu_list)})\n",
    "    return_dict.update({'total_memory': convert_size(np.max(total_memory_list))})\n",
    "    return_dict.update({'max_used_memory': convert_size(np.max(used_memory_list))})\n",
    "    return_dict.update({'max_active_memory': convert_size(np.max(active_memory_list))})\n",
    "    return_dict.update({'max_buffered_memory': convert_size(np.max(buffered_memory_list))})\n",
    "    return_dict.update({'max_cached_memory': convert_size(np.max(cached_memory_list))})\n",
    "    return_dict.update({'max_shared_memory': convert_size(np.max(shared_memory_list))})\n",
    "    return_dict.update({'max_swapped_memory': convert_size(np.max(swap_memory_list))})\n",
    "    return_dict.update({'max_thread_memory': max_memory})\n",
    "\n",
    "    \n",
    "    write_dict(return_dict, context['summary'], 'Maximum Usage Stats')\n",
    "    pickled = pickler(context['pickle'], return_dict, 'max stats')\n",
    "    \n",
    "    return return_dict\n",
    "\n",
    "\n",
    "\n",
    "#sends email to self from self, with passed subject and body\n",
    "#Files to attach can be passed as list to the 'files' argument\n",
    "\n",
    "def send_email(subject, body, version_list_html='', files=None, context = None):\n",
    "    \n",
    "    def prompt(prompt):\n",
    "        return raw_input(prompt).strip()\n",
    "\n",
    "    fromaddr = 'abhijeet.jha@gmail.com'\n",
    "    toaddr  = 'abhijeet.jha@gmail.com'\n",
    "    msg = MIMEMultipart()\n",
    "    msg['From'] = fromaddr\n",
    "    msg['To'] = toaddr\n",
    "    msg['Subject'] = subject\n",
    "    \n",
    "    body = body\n",
    "    \n",
    "    msg.attach(MIMEText(body, 'html'))\n",
    "    \n",
    "    \n",
    "    footer =\"<br>><hr>\" + version_list_html\n",
    "    msg.attach(MIMEText(footer, 'html'))\n",
    "\n",
    "    \n",
    "    #######################################\n",
    "#     To embed accuracy image\n",
    "#     pickled = pickler(context['pickle'])\n",
    "#     img = pickled['accuracy plot']\n",
    "\n",
    "\n",
    "\n",
    "#     # This example assumes the image is in the current directory\n",
    "#     fp = open(img, 'rb')\n",
    "#     msgImage = MIMEImage(fp.read())\n",
    "#     fp.close()\n",
    "\n",
    "#     # Define the image's ID as referenced above\n",
    "#     msgImage.add_header('Content-ID', '<image1>')\n",
    "#     msg.attach(msgImage)\n",
    "\n",
    "\n",
    "####################################\n",
    "    for f in files or []:\n",
    "        with open(f, \"rb\") as fil:\n",
    "            part = MIMEApplication(\n",
    "                fil.read(),\n",
    "                Name=basename(f)\n",
    "            )\n",
    "            part['Content-Disposition'] = 'attachment; filename=\"%s\"' % basename(f)\n",
    "            msg.attach(part)\n",
    "\n",
    "    \n",
    " \n",
    "    smtp_server = 'email-smtp.us-east-1.amazonaws.com'\n",
    "    smtp_username = 'AKIAJFYKGSZH6TNFD2WQ'\n",
    "    smtp_password = 'AoSGycN2iVoV9b/eDhm6ht2ZK7OaRa58InGKywLQ/nfF'\n",
    "    smtp_port = '587'\n",
    "    smtp_do_tls = True\n",
    "\n",
    "    server = smtplib.SMTP(\n",
    "        host = smtp_server,\n",
    "        port = smtp_port,\n",
    "        timeout = 10\n",
    "        )\n",
    "    server.starttls()\n",
    "    server.ehlo()\n",
    "    server.login(smtp_username, smtp_password)\n",
    "    \n",
    "    text = msg.as_string()\n",
    "    server.sendmail(fromaddr, toaddr, text)\n",
    "\n",
    "    \n",
    "# create html markup for a dictionay. \n",
    "# Note - doesnt work with nested dictionaries\n",
    "#TO DO - write an iterator \n",
    "def dict_to_html(dict):\n",
    "    df=pd.DataFrame(dict)\n",
    "    outhtml= df.to_html(na_rep = \"\", index = True).replace('border=\"1\"','border=\"0\"')\n",
    "    outhtml=outhtml.replace('<th>','<th style = \"display: none\">')\n",
    "    outhtml=outhtml.replace('<td>','<td style= \"padding: 8px;text-align: left;border-bottom: 1px solid #ddd;;\">')\n",
    "    outhtml=outhtml.replace('table','table width = \"100%\"')\n",
    "    return outhtml\n",
    "\n",
    "\n",
    "def convert_size(size_bytes):\n",
    "   if size_bytes == 0:\n",
    "       return \"0B\"\n",
    "   size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\n",
    "   i = int(math.floor(math.log(size_bytes, 1024)))\n",
    "   p = math.pow(1024, i)\n",
    "   s = round(size_bytes / p, 2)\n",
    "   return \"%s %s\" % (s, size_name[i])\n",
    "\n",
    "# dataset and labels are of type np.ndarray, returned by merge_dataset()\n",
    "def randomize(dataset, labels):\n",
    "  permutation = np.random.permutation(labels.shape[0])\n",
    "  shuffled_dataset = dataset[permutation,:,:]\n",
    "  shuffled_labels = labels[permutation]\n",
    "  return shuffled_dataset, shuffled_labels\n",
    "\n",
    "\n",
    "def fetch_paths():\n",
    "    today = datetime.date.today().strftime(\"%Y%m%d\")\n",
    "    now = time.strftime(\"%H%M%S\",time.gmtime())\n",
    "\n",
    "    \n",
    "    model_path = 'savedmodels/' + today +'/'\n",
    "    log_path = 'logs/' + today +'/'\n",
    "    stats_path = 'stats/' + today +'/'\n",
    "    runprofiles_path = 'runprofiles/' + today +'/'\n",
    "    pickle_path = 'runpickles/' + today + '/'\n",
    "    plot_path = 'plots/' + today + '/' + 'run_' + now +'/'\n",
    "    \n",
    "    current = str(os.getcwd())\n",
    "    log_root = os.path.join(log_path)\n",
    "    model_root = os.path.join(model_path)\n",
    "    stats_root = os.path.join(stats_path)\n",
    "    runprofiles_root = os.path.join(runprofiles_path)\n",
    "    pickled_root = os.path.join(pickle_path)\n",
    "    plot_root = os.path.join(plot_path)\n",
    "\n",
    "    if not os.path.exists(model_root):\n",
    "        os.makedirs(model_root)\n",
    "\n",
    "    if not os.path.exists(log_root):\n",
    "        os.makedirs(log_root)\n",
    "        \n",
    "    if not os.path.exists(stats_root):\n",
    "        os.makedirs(stats_root)\n",
    "        \n",
    "    if not os.path.exists(runprofiles_root):\n",
    "        os.makedirs(runprofiles_root)\n",
    "    \n",
    "    if not os.path.exists(pickled_root):\n",
    "        os.makedirs(pickled_root)\n",
    "    \n",
    "    if not os.path.exists(plot_root):\n",
    "        os.makedirs(plot_root)\n",
    "        \n",
    "    summary = runprofiles_root + 'summary_' + today + now + '.txt'\n",
    "    pickle = pickled_root + 'run_' + today + now \n",
    "    modelpickles = model_root + 'pickled_' + today + now\n",
    "    statsfile = stats_root + 'stats_' + today + now\n",
    "    \n",
    "    context ={}\n",
    "    context.update({'log path': log_root})\n",
    "    context.update({'plot path': plot_root})\n",
    "    context.update({'model path': model_root})\n",
    "    context.update({'stats path': stats_root})\n",
    "    context.update({'runprofiles path': runprofiles_root})\n",
    "    context.update({'run date': today})\n",
    "    context.update({'run time': now})\n",
    "    context.update({'summary': summary})\n",
    "    context.update({'pickle': pickle})\n",
    "    context.update({'modelpickles': modelpickles})\n",
    "    context.update({'statsfile': statsfile})\n",
    "    \n",
    "    return context\n",
    "\n",
    "\n",
    "def html_class_name(class_name):\n",
    "    #class_name = class_name.replace(\"<class '\", \"\")\n",
    "    class_name = class_name.replace(\">\", \"\")\n",
    "    class_name = class_name.replace(\"<\", \"\")\n",
    "    class_name = class_name.replace(\"'\", \"\")\n",
    "    class_name = class_name.replace(\" \", \"\")\n",
    "    class_name = class_name.replace(\".\", \"\")\n",
    "    class_name = class_name.replace(\":\", \"\")\n",
    "    return class_name\n",
    "\n",
    "\n",
    "\n",
    "# Routine to add commas to a float string\n",
    "def commify3(amount):\n",
    "    amount = str(amount)\n",
    "    amount = amount[::-1]\n",
    "    amount = re.sub(r\"(\\d\\d\\d)(?=\\d)(?!\\d*\\.)\", r\"\\1,\", amount)\n",
    "    return amount[::-1]\n",
    "\n",
    "\n",
    "\n",
    "def save_summary(context, stats_file_path):\n",
    "    #print (\" --------------------------------------------------------------------\")\n",
    "    #summary = context['runprofiles path'] + 'summary_'+ context['run time'] +'.txt'\n",
    "    stream = open(os.path.join(context['summary']), 'a');\n",
    "    stats = pstats.Stats(stats_file_path, stream=stream)\n",
    "    pprint.pformat(stats.strip_dirs().sort_stats('cumtime').print_stats(15))\n",
    "    stream.flush()\n",
    "    stream.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def poll_system_profile(context, interval=0.0):\n",
    "    #log_root, model_root, stats_root, today, now = fetch_paths()    \n",
    "    num_cpu =psutil.cpu_count()\n",
    "    percpu_list =[]\n",
    "    \n",
    "    # Current system-wide CPU utilization as a percentage\n",
    "    # ---------------------------------------------------\n",
    " \n",
    "    # Individual CPUs\n",
    "    sys_percs_percpu = psutil.cpu_percent(interval, percpu=True)\n",
    "    \n",
    "    \n",
    "    for cpu_num, perc in enumerate(sys_percs_percpu):\n",
    "        percpu_list.append(perc)\n",
    "    # end for\n",
    " \n",
    " \n",
    "    # Details on Current system-wide CPU utilziation as a percentage\n",
    " \n",
    "    # --------------------------------------------------------------\n",
    "    # Server as a whole\n",
    "    overall_cpu = np.mean(percpu_list)\n",
    "    sys_percs_total_details = psutil.cpu_times_percent(interval, percpu=False)\n",
    "    mem = psutil.virtual_memory()\n",
    "    swap = psutil.swap_memory()\n",
    "    used = mem.total - mem.available\n",
    "    sys_cpu_times = {}\n",
    "    \n",
    "    sys_cpu_times.update({'profile_time': datetime.date.today().strftime(\"%Y%m%d\") + time.strftime(\"%H%M%S\",time.gmtime()) })\n",
    "    sys_cpu_times.update({'all_cpu': overall_cpu})\n",
    "    sys_cpu_times.update({'per_cpu': sys_percs_percpu})\n",
    "    sys_cpu_times.update({'memory': mem})\n",
    "    sys_cpu_times.update({'swap':swap})\n",
    "    \n",
    "    write_dict(sys_cpu_times, context['summary'], 'Usage Logging')\n",
    "    \n",
    "\n",
    "    \n",
    "    return sys_cpu_times\n",
    "\n",
    "   \n",
    "\n",
    "def measure_memory_usage(context, target_call, target_args, log_interval=30, log_filename=None, memory_usage_refresh=0.01):\n",
    "    \"\"\"\n",
    "    measure the memory usage of a function call in python.\\n\n",
    "    Note: one may have to restart python to get accurate results.\\n\n",
    "    :param target_call: function to be tested\\n\n",
    "    :param target_args: arguments of the function in a tuple\\n\n",
    "    :param memory_usage_refresh: how frequent the memory is measured, default to 0.005 seconds\\n\n",
    "    :return: max memory usage in kB (on Linux/Ubuntu 14.04), may depend on OS\n",
    "    \"\"\"\n",
    "  \n",
    "\n",
    "    class StoppableThread(threading.Thread):\n",
    "        def __init__(self, target, args):\n",
    "            super(StoppableThread, self).__init__(target=target, args=args)\n",
    "            self.daemon = True\n",
    "            self.__monitor = threading.Event()\n",
    "            self.__monitor.set()\n",
    "            self.__has_shutdown = False\n",
    "\n",
    "        def run(self):\n",
    "            '''Overloads the threading.Thread.run'''\n",
    "            # Call the User's Startup functions\n",
    "            self.startup()\n",
    "\n",
    "            # use the run method from Superclass threading.Thread\n",
    "            super(StoppableThread, self).run()\n",
    "\n",
    "            # Clean up\n",
    "            self.cleanup()\n",
    "\n",
    "            # Flag to the outside world that the thread has exited\n",
    "            # AND that the cleanup is complete\n",
    "            self.__has_shutdown = True\n",
    "\n",
    "        def stop(self):\n",
    "            self.__monitor.clear()\n",
    "\n",
    "        def isRunning(self):\n",
    "            return self.__monitor.isSet()\n",
    "\n",
    "        def isShutdown(self):\n",
    "            return self.__has_shutdown\n",
    "\n",
    "        def mainloop(self):\n",
    "            '''\n",
    "            Expected to be overwritten in a subclass!!\n",
    "            Note that Stoppable while(1) is handled in the built in \"run\".\n",
    "            '''\n",
    "            pass\n",
    "\n",
    "        def startup(self):\n",
    "            '''Expected to be overwritten in a subclass!!'''\n",
    "            pass\n",
    "\n",
    "        def cleanup(self):\n",
    "            '''Expected to be overwritten in a subclass!!'''\n",
    "            pass\n",
    "\n",
    "    class MyLibrarySniffingClass(StoppableThread):\n",
    "        def __init__(self, target, args):\n",
    "            super(MyLibrarySniffingClass, self).__init__(target=target, args=args)\n",
    "            self.target_function = target\n",
    "            self.results = None\n",
    "\n",
    "        def startup(self):\n",
    "            # Overload the startup function\n",
    "            print (\"Calling the Target Library Function...\")\n",
    "\n",
    "        def cleanup(self):\n",
    "            # Overload the cleanup function\n",
    "            print (\"Library Call Complete\")\n",
    "\n",
    "        #process = psutil.Process(os.getpid())\n",
    "\n",
    "   \n",
    "    process = psutil.Process(os.getpid())\n",
    "    my_thread = MyLibrarySniffingClass(target_call, target_args)\n",
    "    \n",
    "    run_profile ={}\n",
    "    start_mem = process.memory_full_info().uss  #uss\n",
    "    \n",
    "    sys_profile = poll_system_profile(context, interval=0.1)\n",
    "    print (\"Written to summary File\")\n",
    "    \n",
    "    run_profile.update({time.strftime(\"%H:%M:%S\",time.gmtime()): sys_profile})\n",
    "    \n",
    "    my_thread.start()\n",
    "    delta_mem = 0\n",
    "    max_memory = 0\n",
    "    last_run=time.time()\n",
    "\n",
    "    while(True):\n",
    "        time.sleep(memory_usage_refresh)\n",
    "        cur_time = time.time()\n",
    "        del_time = cur_time - last_run\n",
    "        \n",
    "        \n",
    "        \n",
    "        if round(del_time) > log_interval:\n",
    "            sys_profile = poll_system_profile(context)\n",
    "            print (\"Written to summary File\")\n",
    "            last_run = cur_time\n",
    "            run_profile.update({time.strftime(\"%H:%M:%S\",time.gmtime()): sys_profile})\n",
    "            #print(run_profile)\n",
    "        \n",
    "        current_mem = process.memory_info().rss \n",
    "        delta_mem = current_mem - start_mem\n",
    "        if delta_mem > max_memory:\n",
    "            max_memory = delta_mem\n",
    "\n",
    "            \n",
    "        if my_thread.isShutdown():\n",
    "            print (\"Memory measurement complete!\")\n",
    "            break\n",
    "\n",
    "    current_mem = process.memory_full_info().uss  #uss\n",
    "    delta_mem = current_mem - start_mem\n",
    "    if delta_mem > max_memory:\n",
    "        max_memory = delta_mem\n",
    "\n",
    "\n",
    "\n",
    "    print (\"MAX Memory Usage in MB: {}\".format( convert_size(max_memory)))\n",
    "\n",
    "    \n",
    "    run_profile.update({time.strftime(\"%H:%M:%S\",time.gmtime()): sys_profile})\n",
    "    run_profile.update({'max_memory': convert_size(max_memory)})\n",
    "   \n",
    "    \n",
    "    written = max_stats(run_profile, context)\n",
    "    \n",
    "    return written\n",
    "\n",
    "\n",
    "\n",
    "def objects_growth(path, description = ''):\n",
    "    \n",
    "    \n",
    "    orig_stdout = sys.stdout\n",
    "    \n",
    "    f = open(path, 'a')\n",
    "    sys.stdout = f\n",
    "    \n",
    "    print(description)\n",
    "    f.flush()\n",
    "    \n",
    "    print(sys.version)\n",
    "    print(\"---------------\")\n",
    "    print(\"Object Growth\")\n",
    "    print(objgraph.show_growth()) \n",
    "    f.flush()\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    sys.stdout = orig_stdout\n",
    "    #return''\n",
    "\n",
    "def modelfit(alg, datasets, labels, context, metrics, useTrainCV=True, cv_folds=3, early_stopping_rounds=20, num_labels = None):\n",
    "      \n",
    "    try:\n",
    "          train_dataset= datasets[0]\n",
    "          train_labels = labels[0]\n",
    "    except Exception as e:\n",
    "          print('Unable to save data to load training samples', e)\n",
    "          raise\n",
    "    \n",
    "    valid_dataset = datasets[1]\n",
    "    test_dataset = datasets[2]\n",
    "    \n",
    "    #train_labels = labels[0]\n",
    "    valid_labels = labels[1]\n",
    "    test_labels = labels[2]\n",
    "\n",
    "    \n",
    "    run_stats={}\n",
    "    optimal_boosters = 0\n",
    "    num_class = num_labels\n",
    "    \n",
    "    \n",
    "    if useTrainCV:\n",
    "\n",
    "        \n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgb_param.update({'num_class': num_class})\n",
    "        run_stats.update({'original parameters': xgb_param})\n",
    "\n",
    "\n",
    "        xgtrain = xgb.DMatrix(train_dataset,label=train_labels)\n",
    "\n",
    "        \n",
    "        cv_start_time = time.time()\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,metrics=metrics, early_stopping_rounds=early_stopping_rounds)\n",
    "        cv_end_time = time.time()\n",
    "        \n",
    "\n",
    "        \n",
    "        cv_time_raw = cv_end_time - cv_start_time\n",
    "        cv_time = time.strftime(\"%H:%M:%S s\",time.gmtime(cv_time_raw))\n",
    "        run_stats.update({'cv run time': cv_time})\n",
    "\n",
    "        \n",
    " \n",
    "        alg.set_params(n_estimators = cvresult.shape[0])\n",
    "        optimal_boosters = cvresult.shape[0]\n",
    "        run_stats.update({'optimal_boosters': optimal_boosters})\n",
    "        \n",
    "    #Fit the algorithm on the data\n",
    "    fit_start_time =time.time()\n",
    "    alg.fit(train_dataset, train_labels,eval_metric=metrics)\n",
    "    fit_end_time =time.time()\n",
    "    \n",
    "    fit_time_raw = fit_end_time - fit_start_time\n",
    "    fit_time = time.strftime(\"%H:%M:%S s\",time.gmtime(fit_time_raw))\n",
    "    run_stats.update({'fit time': fit_time})\n",
    "    #print(run_stats)\n",
    "        \n",
    "    #Predict training and validation set:\n",
    "    predict_start_time = time.time()\n",
    "    dtrain_predictions = alg.predict(train_dataset)\n",
    "    dvalid_predictions = alg.predict(valid_dataset)\n",
    "    dtest_predictions = alg.predict(test_dataset)\n",
    "    predict_end_time = time.time()\n",
    "    \n",
    "    predict_time_raw = predict_end_time - predict_start_time\n",
    "    predict_time = time.strftime(\"%H:%M:%S s\",time.gmtime(predict_time_raw))\n",
    "    run_stats.update({'predict time': predict_time})\n",
    "    #print(run_stats)\n",
    "        \n",
    "     #Print model report:\n",
    "    acc_score_train = accuracy_score(train_labels, dtrain_predictions)\n",
    "    acc_score_valid = accuracy_score(valid_labels, dvalid_predictions)\n",
    "    acc_score_test = accuracy_score(test_labels, dtest_predictions)\n",
    "    print (\"\\nModel Report\")\n",
    "    print (\"Accuracy : {0:.5f}\".format(acc_score_train)) \n",
    "    print (\"Optimal Boosters : {}\".format(optimal_boosters)) \n",
    "    \n",
    "    run_stats.update({' Train Accuracy': acc_score_train})\n",
    "    if acc_score_valid: run_stats.update({' Validation Accuracy': acc_score_valid})\n",
    "    if acc_score_test: run_stats.update({' Test Accuracy': acc_score_test})\n",
    "\n",
    "    booster = alg.booster()\n",
    "    fit_parameters = booster.attributes()\n",
    "    run_stats.update({'fit attributes': fit_parameters})\n",
    "    class_name = html_class_name(str(booster.__class__))\n",
    "    \n",
    "    #print(now)\n",
    "    fname = context['model path'] + str(class_name) + context['run date'] + context['run time']\n",
    "    \n",
    "    \n",
    "    alg.booster().save_model(fname)\n",
    "    run_stats.update({'saved model path': fname})\n",
    "    pickled = pickler(context['modelpickles'], alg, 'model')\n",
    "    run_stats.update({'pickled model': context['modelpickles']})\n",
    "    \n",
    "    feat_imp_ser = pd.Series(alg.booster().get_fscore()).head(10).sort_values(ascending=False)\n",
    "    feat_dict = feat_imp_ser.to_dict()\n",
    "    run_stats.update({'Feature Importance Score': feat_dict})\n",
    "    #print(run_stats)\n",
    "     \n",
    "    write_dict(run_stats, context['summary'], '#Booster Optimize Run')\n",
    "    pickled = pickler(context['pickle'], run_stats, 'model results')\n",
    "    \n",
    "    #plotCV(cvresult, acc_score_train, acc_score_valid)  \n",
    "    plotCV(cvresult, optimal_boosters, context, acc_score_train, acc_score_valid, acc_score_test)\n",
    "    \n",
    "    \n",
    "    ##########Book keeping - update optimal parameters in dictionary with new boosters\n",
    "    \n",
    "    parameters = xgb_param\n",
    "\n",
    "    #native xgboost requires num_class, scikit_learn doesnt like it\n",
    "    del parameters['num_class']\n",
    "\n",
    "    #update with results\n",
    "    parameters.update({'n_estimators': optimal_boosters})\n",
    "\n",
    "    updated_pickle =pickler(context['pickle'], parameters, 'optimal parameters')\n",
    "    \n",
    "    return updated_pickle\n",
    "    ########## End Book Keeping\n",
    " \n",
    "\n",
    "\n",
    "def plotCV(cvresult, optimal_boosters, context, accuracy_train = 0, accuracy_valid = 0, accuracy_test = 0,  title ='Accuracy Score by Tree Growth', ylim=(0.7,1)):\n",
    "    # ylim=(0.8,1.01)\n",
    "    \n",
    "    plt.rcParams['figure.figsize'] = (20,10)\n",
    "    cvresult_df = pd.DataFrame(cvresult)\n",
    "    x_values = list(range(cvresult_df.shape[0]))\n",
    "    test_error = cvresult_df.iloc[:,0].tolist()\n",
    "    test_std = cvresult_df.iloc[:,1].tolist()\n",
    "    \n",
    "    train_error = cvresult_df.iloc[:,2].tolist()\n",
    "    train_std = cvresult_df.iloc[:,3].tolist()\n",
    "    \n",
    "    x_values_int= [None]*len(x_values)\n",
    "    test_error_float= [None]*len(x_values)\n",
    "    test_std_float= [None]*len(x_values)\n",
    "    train_error_float= [None]*len(x_values)\n",
    "    train_std_float= [None]*len(x_values)\n",
    "    \n",
    "    for i in range(len(x_values)):\n",
    "        x_values_int[i] = int(x_values[i])\n",
    "        test_error_float[i] = 1 - float(test_error[i])\n",
    "        test_std_float[i] = float(test_std[i])\n",
    "        train_error_float[i] = 1 - float(train_error[i])\n",
    "        train_std_float[i] = float(train_std[i])\n",
    "        \n",
    "      \n",
    "    fig = plt.figure()\n",
    "    plt.xlabel('Number of Boosters')\n",
    "    plt.ylabel('Accuracy')\n",
    "    #plt.ylim(0.7,1,1)\n",
    "\n",
    "    plt.plot(x_values_int,\n",
    "         train_error_float,\n",
    "         label='Training Score',\n",
    "         color = 'r')\n",
    "\n",
    "    plt.plot(x_values_int,\n",
    "         test_error_float,\n",
    "         label='CV Score',\n",
    "         color = 'g')\n",
    "\n",
    "\n",
    "    plt.fill_between(x_values_int,\n",
    "                np.array(train_error_float) - np.array(train_std_float),\n",
    "                np.array(train_error_float) + np.array(train_std_float),\n",
    "                alpha =0.2, color ='r')\n",
    "\n",
    "    plt.fill_between(x_values_int,\n",
    "                np.array(test_error_float) - np.array(test_std_float),\n",
    "                np.array(test_error_float) + np.array(test_std_float),\n",
    "                alpha =0.2, color ='g')\n",
    "\n",
    "\n",
    "    plt.axhline(y = 1, color='k', ls ='dashed')\n",
    "    plt.axvline(x = optimal_boosters, ls ='dashed', label ='Optimal #Boosters')\n",
    "\n",
    "    plt.plot(optimal_boosters, float(accuracy_train), marker='o', markersize=6, color=\"blue\", label = 'Train Accuracy')\n",
    "    plt.plot(optimal_boosters, float(accuracy_valid), marker='o', markersize=6, color=\"maroon\", label = 'Valid Accuracy')\n",
    "    plt.plot(optimal_boosters, float(accuracy_test), marker='3', markersize=6, color=\"green\", label = 'Test Accuracy')\n",
    "    \n",
    "    plt.text(optimal_boosters+5, float(accuracy_train), float(accuracy_train), fontsize =12, \n",
    "             bbox=dict(facecolor='none', edgecolor='blue', boxstyle='round,pad=1'))\n",
    "    \n",
    "    plt.text(optimal_boosters+5, float(accuracy_valid), float(accuracy_valid), fontsize =12, \n",
    "             bbox=dict(facecolor='none', edgecolor='maroon', boxstyle='round,pad=1'))\n",
    "    \n",
    "    plt.text(optimal_boosters+5, float(accuracy_test), float(accuracy_test), fontsize =12, \n",
    "             bbox=dict(facecolor='none', edgecolor='green', boxstyle='round,pad=1'))\n",
    "    \n",
    "    \n",
    "    plt.legend(loc = 'best')\n",
    "    if ylim:\n",
    "        plt.ylim(ylim)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    now = time.strftime(\"%H%M%S\",time.gmtime())\n",
    "    save_file = context['plot path'] + now + '.png'\n",
    "    fig.savefig(save_file, bbox_inches='tight')\n",
    "    pickled = pickler(context['pickle'], save_file, 'accuracy plot')\n",
    "\n",
    "\n",
    "def extend_single_param(result, delta_step, allowed_range, seen):\n",
    "    \n",
    "    new_list ={'left': round(result - delta_step, 2), 'right': round(result + delta_step, 2)}\n",
    "    \n",
    "    \n",
    "    if new_list['left'] <= allowed_range[0] or new_list['left'] in seen:\n",
    "        del new_list['left']\n",
    "    \n",
    "    if new_list['right'] > allowed_range[1] or new_list['right'] in seen:\n",
    "        del new_list['right']\n",
    "    \n",
    "    return list(new_list.values())\n",
    "\n",
    "\n",
    "\n",
    "def extend_param_dict(current_best, steps, allowed_ranges, seen):\n",
    "    #first find the extended range for each parameter\n",
    "    # step and allowed range and dictionaries for values for each tunable parameters\n",
    "    parameters ={}\n",
    "    for k, v in current_best.items():\n",
    "        seen_list = seen[k]\n",
    "        step =steps[k]\n",
    "        allowed_range = allowed_ranges[k]\n",
    "        print('parameter', k)\n",
    "        print('result:',v,' step:', step,' allowed_range:', allowed_range,' seen:', seen_list)\n",
    "        new_range = extend_single_param( v, step, allowed_range, seen_list)\n",
    "        print(new_range)\n",
    "        parameters.update({k:new_range})\n",
    "    \n",
    "    #iterate through new parameters - if none, set to incoming current best value\n",
    "    \n",
    "    for k, v in parameters.items():\n",
    "        if not v: parameters[k] = [current_best[k]]\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "\n",
    "def remove_duplicates(inlist):\n",
    "    outlist =[]\n",
    "    for i in inlist:\n",
    "        if i not in outlist:\n",
    "            outlist.append(float(i))\n",
    "    \n",
    "    return outlist\n",
    "    \n",
    "\n",
    "def tuner_cv(train_set, train_labels, val_set, val_labels, param_test, tuning_rounds, steps, allowed_ranges, context, scoring ='accuracy', cv = 3, val_tuned =True):\n",
    "    pickled = pickler(context['pickle'])\n",
    "    parameters = pickled['optimal parameters']\n",
    "    \n",
    "    tuning_results_params ={}\n",
    "    tuning_results_accuracy ={}\n",
    "    tuning_validation_accuracy ={}\n",
    "    rounds_to_tune = tuning_rounds\n",
    "\n",
    "    current_tuning_round = 0\n",
    "    estimator = XGBClassifier(**parameters)\n",
    "\n",
    "    tuned = False\n",
    "    param_test = param_test\n",
    "    seen = param_test\n",
    "\n",
    "\n",
    "    while not tuned:\n",
    "    \n",
    "        loop_result =()\n",
    "        \n",
    "        #update seen with parameters already tested\n",
    "        seen = { k:  seen[k] + param_test[k]  for k in seen }\n",
    "        seen = { k:  remove_duplicates(seen[k]) for k in seen }\n",
    "        \n",
    "        # Remove the duplicates\n",
    "        #seen = list(set(seen))\n",
    "    \n",
    "        gsearch = GridSearchCV(estimator = estimator, \n",
    "                        param_grid = param_test, \n",
    "                        scoring= scoring,\n",
    "                        n_jobs= -1,\n",
    "                        cv= cv)\n",
    "\n",
    "        loop_result = gsearch.fit(train_set, train_labels)\n",
    "        \n",
    "        # score on the validation dataset\n",
    "        loop_result_val = loop_result.score(val_set, val_labels)\n",
    "\n",
    "        tuning_results_params.update({'iter'+str(current_tuning_round): loop_result.best_params_ })\n",
    "        tuning_results_accuracy.update({'iter'+str(current_tuning_round): loop_result.best_score_ })\n",
    "        tuning_validation_accuracy.update({'iter'+str(current_tuning_round): loop_result_val })\n",
    "    \n",
    "        print('Current Iteration ', loop_result.best_params_ , ' CV Accuracy ', loop_result.best_score_, ' Validation Accuracy ', loop_result_val)\n",
    "    \n",
    "    \n",
    "        current_tuning_round = current_tuning_round + 1\n",
    "    \n",
    "        param_test = extend_param_dict(loop_result.best_params_, steps, allowed_ranges, seen)\n",
    "        print(\"Extended List :\", param_test)\n",
    "        print('-------------------------------')\n",
    "    \n",
    "        #convert result dict values into list for comparison\n",
    "        best_params_list ={k: [loop_result.best_params_ [k]] for k in loop_result.best_params_ }\n",
    "        if param_test == best_params_list : tuned = True\n",
    "        if current_tuning_round == rounds_to_tune:  tuned = True\n",
    "    \n",
    "\n",
    "    \n",
    "    ##END WHILE TUNED  \n",
    "    write_dict(seen, context['summary'],'Tested Values')\n",
    "\n",
    "    #prepare dict for writing to file\n",
    "    tuner_results_summary ={key: str(tuning_results_params[key]) + '  CV Accuracy: ' + str(tuning_results_accuracy[key]) + '  Validation Accuracy: ' + str(tuning_validation_accuracy[key]) for key in tuning_results_params.keys() }\n",
    "\n",
    "    #compute the highest  CV accuracy \n",
    "    max_accuracy_key =max(tuning_results_accuracy, key=lambda key: tuning_results_accuracy[key])  \n",
    "    \n",
    "    #compute the highest Validation accuracy \n",
    "    if val_tuned: max_accuracy_key =max(tuning_validation_accuracy, key=lambda key: tuning_validation_accuracy[key])  \n",
    "\n",
    "    \n",
    "    # use CV accuracy for tuning\n",
    "    #tuning_results_params[max_accuracy_key]\n",
    "    \n",
    "    # use Validation accuracy for traiing\n",
    "    tuning_results_params[max_accuracy_key]\n",
    "\n",
    "    #pprint.pprint(tuner_results_summary)\n",
    "    write_dict(tuner_results_summary, context['summary'], 'Tuning Iterations')\n",
    "    write_dict({'Chosen:': str(tuning_results_params[max_accuracy_key]) + ' CV Accuracy: ' + str(tuning_results_accuracy[max_accuracy_key]) + ' Validation Accuracy: ' + str(tuning_validation_accuracy[max_accuracy_key])}, context['summary'])\n",
    "    \n",
    "\n",
    "    # Get the optimal parameters from the run\n",
    "    \n",
    "    # use Validation accuracy\n",
    "    params_to_update = tuning_results_params[max_accuracy_key]\n",
    "\n",
    "    # Update the parameters list with the new updated values for the params tested\n",
    "    parameters.update({k: params_to_update[k] for k in params_to_update.keys()})\n",
    "\n",
    "    # Update the pickle\n",
    "    updated_pickle = pickler(context['pickle'], parameters, 'optimal parameters')\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
